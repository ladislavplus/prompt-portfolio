import argparse
import json
import pandas as pd
from datetime import datetime
import os
import sys

from portfolib.utils import load_json
from portfolib.models import get_model_name, run_prompt
from portfolib.evaluation import score_similarity, evaluate_run
from portfolib.reporting import (
    save_results_csv,
    plot_benchmark_results,
    write_benchmark_report,
    plot_comparison_results,
    write_run_report,
    plot_run_results,
    plot_multi_model_comparison_results,
    write_multi_model_comparison_report,
)

def list_available_files(directory, extension=".json"):
    """Scans a directory for files with a given extension."""
    if not os.path.isdir(directory):
        return []
    return [f for f in os.listdir(directory) if f.endswith(extension)]

def list_available_models():
    """Loads model aliases from the configuration file."""
    models_config = load_json("config/models_config.json")
    return list(models_config["models"].keys())

def benchmark_command(args):
    """Runs the benchmark command."""
    print(f"ðŸš€ Running benchmark for prompt '{args.prompt}' with model '{args.model_alias}'...")
    
    models_config = load_json("config/models_config.json")
    model_name = get_model_name(args.model_alias, models_config)
    
    prompt_data = load_json(args.prompt)
    test_data = load_json(args.benchmark)

    results = []
    for case in test_data["test_cases"]:
        output = run_prompt(prompt_data["prompt_text"], model_name, case["input"])
        score = score_similarity(case["expected_output"], output)
        results.append({
            "id": case["id"],
            "input": json.dumps(case["input"]),
            "expected": case["expected_output"],
            "output": output,
            "similarity": score,
            "model_alias": args.model_alias,
        })

    if not results:
        print("âŒ No results were generated. Exiting.")
        return

    df = pd.DataFrame(results)
    avg_similarity = df["similarity"].mean()
    df["avg_similarity"] = avg_similarity

    prompt_name_slug = prompt_data['name'].replace('_', '-')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"{test_data['name']}_{prompt_name_slug}_{args.model_alias}_{timestamp}.csv"
    save_results_csv(df, args.output_dir, csv_filename)

    print(f"âœ… Benchmark complete. Average similarity: {avg_similarity:.3f}")

    chart_path = plot_benchmark_results(df, prompt_data['name'], test_data['name'], args.model_alias, timestamp, args.media_dir)
    write_benchmark_report(df, prompt_data['name'], test_data['name'], args.model_alias, timestamp, chart_path)

def compare_command(args):
    """Runs the compare command."""
    print(f"ðŸš€ Comparing prompts from project '{args.project}'...")

    project_data = load_json(args.project)
    prompt_files = project_data["prompts"]
    all_results = []

    # Create a temporary namespace to hold arguments for benchmark_command
    benchmark_args = argparse.Namespace(**vars(args))

    for prompt_file in prompt_files:
        benchmark_args.prompt = prompt_file
        
        # Temporarily redirect output to avoid clutter
        from io import StringIO
        old_stdout = sys.stdout
        sys.stdout = StringIO()

        try:
            benchmark_command(benchmark_args)
        finally:
            sys.stdout.seek(0)
            # output = sys.stdout.read() # This is not used, so commented out
            sys.stdout = old_stdout
        
        # Find the latest result file for this prompt
        list_of_files = os.listdir(args.output_dir)
        test_data = load_json(args.benchmark)
        prompt_data = load_json(prompt_file)
        prompt_name_slug = prompt_data['name'].replace('_', '-')
        
        # Correctly find files generated by the benchmark command
        file_prefix = f"{test_data['name']}_{prompt_name_slug}_{args.model_alias}"
        full_path = [os.path.join(args.output_dir, i) for i in list_of_files if i.startswith(file_prefix)]

        if not full_path:
            print(f"Could not find results file for {prompt_file} with prefix '{file_prefix}'")
            continue
            
        latest_file = max(full_path, key=os.path.getctime)
        df = pd.read_csv(latest_file)
        df["prompt_name"] = load_json(prompt_file)["name"]
        all_results.append(df)


    if not all_results:
        print("âŒ No results were generated from any prompt. Exiting.")
        return

    combined = pd.concat(all_results)
    
    project_name_slug = project_data['name'].replace('_', '-')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    combined_csv_filename = f"comparison_{project_name_slug}_{args.model_alias}_{timestamp}.csv"
    save_results_csv(combined, args.output_dir, combined_csv_filename)

    avg_df = combined.groupby("prompt_name")["similarity"].mean().reset_index()
    
    print("\nðŸ“Š Average similarity per prompt:")
    print(avg_df.to_string(index=False))
    
    plot_comparison_results(avg_df, project_data['name'], args.model_alias, timestamp, args.media_dir)

def compare_models_command(args):
    """Compares a single prompt across multiple models."""
    print(f"ðŸš€ Comparing prompt '{args.prompt}' across models: {args.model_aliases}...")

    prompt_data = load_json(args.prompt)
    test_data = load_json(args.benchmark)
    models_config = load_json("config/models_config.json")

    all_results = []
    for model_alias in args.model_aliases:
        print(f"Benchmarking with model: '{model_alias}'")
        model_name = get_model_name(model_alias, models_config)
        
        results = []
        for case in test_data["test_cases"]:
            output = run_prompt(prompt_data["prompt_text"], model_name, case["input"])
            score = score_similarity(case["expected_output"], output)
            results.append({
                "id": case["id"],
                "input": json.dumps(case["input"]),
                "expected": case["expected_output"],
                "output": output,
                "similarity": score,
                "model_alias": model_alias,
            })
        
        all_results.extend(results)

    if not all_results:
        print("âŒ No results were generated from any model. Exiting.")
        return

    df = pd.DataFrame(all_results)
    
    prompt_name_slug = prompt_data['name'].replace('_', '-')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"model_comparison_{prompt_name_slug}_{timestamp}.csv"
    save_results_csv(df, args.output_dir, csv_filename)

    avg_df = df.groupby("model_alias")["similarity"].mean().reset_index()

    print("\nðŸ“Š Average similarity per model:")
    print(avg_df.to_string(index=False))

    chart_path = plot_multi_model_comparison_results(avg_df, prompt_data['name'], timestamp, args.media_dir)
    write_multi_model_comparison_report(avg_df, prompt_data['name'], timestamp, chart_path, "output/summaries")


def run_command(args):
    """Runs the run command."""
    models_config = load_json("config/models_config.json")
    model_alias = args.model_alias or models_config["default_model"]
    model_name = get_model_name(model_alias, models_config)

    print(f"ðŸš€ Running prompts with model: {model_alias} ({model_name})")

    data = pd.read_json(args.data)
    results = []
    for _, row in data.iterrows():
        prompt = f"Task: {row['task']}\nInput: {row['input']}"
        if "target_lang" in row and row["target_lang"]:
            prompt += f"\nTarget language: {row['target_lang']}"
        
        output = run_prompt(prompt, model_name)
        eval_scores = evaluate_run(output, prompt)
        
        results.append({
            "id": row["id"],
            "task": row["task"],
            "output": output,
            "score": eval_scores["avg_score"],
            "factual": eval_scores["clarity"] > 3.5, # Example logic
            "model_alias": model_alias,
        })

    df = pd.DataFrame(results)
    save_results_csv(df, "output/raw", "prompt_runner_results.csv")
    write_run_report(df, model_alias, model_name, "output/summaries")
    plot_run_results(df, model_alias, "media")

def main():
    parser = argparse.ArgumentParser(
        description="""Prompt Portfolio CLI
        
A tool for managing, testing, and evaluating language model prompts. 
This CLI allows you to run experiments, benchmark prompts, and compare their performance in a systematic way.
""",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""Examples:
  # Benchmark a single prompt
  python cli.py benchmark --prompt prompts/library/translation_prompt.json --benchmark prompts/benchmarks/translation_testset.json --model-alias gpt-4

  # Compare multiple prompts
  python cli.py compare --project experiments/prompt-comparison/prompts_to_compare.json --benchmark prompts/benchmarks/translation_testset.json --model-alias gpt-4

  # Compare a single prompt across multiple models
  python cli.py compare-models --prompt prompts/library/translation_prompt.json --benchmark prompts/benchmarks/translation_testset.json --model-aliases gpt-4 gpt-3.5-turbo

  # Run a dataset of prompts
  python cli.py run --data datasets/sample_inputs.json --model-alias gpt-4
"""
    )
    subparsers = parser.add_subparsers(dest="command", required=True, help="Available commands")

    # Benchmark command
    parser_benchmark = subparsers.add_parser("benchmark", help="Run a benchmark for a single prompt.", epilog="Example: python cli.py benchmark --prompt <path> --benchmark <path> --model-alias <alias>")
    parser_benchmark.add_argument("--benchmark", help="Path to the benchmark testset JSON file.", required=True)
    parser_benchmark.add_argument("--prompt", help="Path to the prompt JSON file.", required=True)
    parser_benchmark.add_argument("--model-alias", help="Alias of the model to use from models_config.json.", required=True)
    parser_benchmark.add_argument("--output-dir", default="output/raw", help="Directory to save the output CSV file.")
    parser_benchmark.add_argument("--media-dir", default="media", help="Directory to save the chart image.")
    parser_benchmark.set_defaults(func=benchmark_command)

    # Compare command
    parser_compare = subparsers.add_parser("compare", help="Compare multiple prompts against a single benchmark.", epilog="Example: python cli.py compare --project <path> --benchmark <path> --model-alias <alias>")
    parser_compare.add_argument("--project", help="Path to the project JSON file listing the prompts to compare.", required=True)
    parser_compare.add_argument("--benchmark", help="Path to the benchmark testset JSON file.", required=True)
    parser_compare.add_argument("--model-alias", help="Alias of the model to use from models_config.json.", required=True)
    parser_compare.add_argument("--output-dir", default="output/raw", help="Directory to save the output CSV files.")
    parser_compare.add_argument("--media-dir", default="media", help="Directory to save the comparison chart.")
    parser_compare.set_defaults(func=compare_command)

    # Compare models command
    parser_compare_models = subparsers.add_parser("compare-models", help="Compare a prompt against multiple models.", epilog="Example: python cli.py compare-models --prompt <path> --benchmark <path> --model-aliases <alias1> <alias2>")
    parser_compare_models.add_argument("--prompt", help="Path to the prompt JSON file.", required=True)
    parser_compare_models.add_argument("--benchmark", help="Path to the benchmark testset JSON file.", required=True)
    parser_compare_models.add_argument("--model-aliases", nargs='+', help="List of model aliases to compare.", required=True)
    parser_compare_models.add_argument("--output-dir", default="output/raw", help="Directory to save the output CSV file.")
    parser_compare_models.add_argument("--media-dir", default="media", help="Directory to save the chart image.")
    parser_compare_models.set_defaults(func=compare_models_command)

    # Run command
    parser_run = subparsers.add_parser("run", help="Run a series of prompts from a dataset.", epilog="Example: python cli.py run --data <path> --model-alias <alias>")
    parser_run.add_argument("--data", default="datasets/sample_inputs.json", help="Path to dataset JSON")
    parser_run.add_argument("--model-alias", help="Alias of the model to use from models_config.json")
    parser_run.set_defaults(func=run_command)

    args = parser.parse_args()
    
    if hasattr(args, 'func'):
        args.func(args)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
